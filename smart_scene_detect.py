#!/usr/bin/env python3
"""
Smart Scene Detection - Improved Automatic TikTok Short Detection

This script uses advanced techniques to automatically detect scene boundaries
between TikTok shorts with better accuracy and no overlap.
"""

import os
import subprocess
import sys
import json
from typing import List, Tuple


def get_video_info(input_video: str) -> dict:
    """Get detailed video information using FFmpeg."""
    cmd = [
        'ffprobe',
        '-v', 'quiet',
        '-print_format', 'json',
        '-show_format',
        '-show_streams',
        input_video
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        data = json.loads(result.stdout)
        
        format_info = data.get('format', {})
        video_stream = next((s for s in data.get('streams', []) if s.get('codec_type') == 'video'), {})
        audio_stream = next((s for s in data.get('streams', []) if s.get('codec_type') == 'audio'), {})
        
        info = {
            'duration': float(format_info.get('duration', 0)),
            'file_size_mb': float(format_info.get('size', 0)) / (1024 * 1024),
            'width': int(video_stream.get('width', 0)),
            'height': int(video_stream.get('height', 0)),
            'fps': eval(video_stream.get('r_frame_rate', '0/1')),
            'audio_fps': int(audio_stream.get('sample_rate', 0)) if audio_stream else None
        }
        
        return info
        
    except subprocess.CalledProcessError as e:
        print(f"Error getting video information: {e}")
        return {}


def detect_scenes_advanced(input_video: str, min_scene_duration: float = 8.0) -> List[float]:
    """
    Advanced scene detection using multiple FFmpeg techniques.
    
    Args:
        input_video: Path to input video file
        min_scene_duration: Minimum duration for a scene in seconds
        
    Returns:
        List of timestamps where scenes change
    """
    print("üîç Detecting scenes using advanced analysis...")
    
    # Method 1: Scene detection using FFmpeg scene filter
    print("  Using FFmpeg scene detection...")
    scene_timestamps = detect_scenes_ffmpeg(input_video, min_scene_duration)
    
    if len(scene_timestamps) < 2:
        print("  ‚ö†Ô∏è  FFmpeg scene detection found too few scenes, trying alternative method...")
        # Method 2: Frame difference analysis
        scene_timestamps = detect_scenes_frame_diff(input_video, min_scene_duration)
    
    if len(scene_timestamps) < 2:
        print("  ‚ö†Ô∏è  Frame difference analysis also failed, using intelligent splitting...")
        # Method 3: Intelligent splitting based on video duration
        scene_timestamps = intelligent_splitting(input_video, min_scene_duration)
    
    return scene_timestamps


def detect_scenes_ffmpeg(input_video: str, min_scene_duration: float) -> List[float]:
    """Detect scenes using FFmpeg's scene filter."""
    try:
        # Use FFmpeg scene filter to detect scene changes
        cmd = [
            'ffmpeg',
            '-i', input_video,
            '-vf', 'select=gt(scene\\,0.4),showinfo',
            '-f', 'null',
            '-'
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        # Parse the output to extract timestamps
        timestamps = []
        for line in result.stderr.split('\n'):
            if 'pts_time:' in line:
                try:
                    # Extract timestamp from FFmpeg output
                    time_str = line.split('pts_time:')[1].split()[0]
                    timestamp = float(time_str)
                    timestamps.append(timestamp)
                except (ValueError, IndexError):
                    continue
        
        # Filter timestamps based on minimum duration
        filtered_timestamps = [0.0]  # Start with first frame
        for ts in timestamps:
            if ts - filtered_timestamps[-1] >= min_scene_duration:
                filtered_timestamps.append(ts)
        
        # Add end time if not already present
        video_info = get_video_info(input_video)
        if filtered_timestamps and filtered_timestamps[-1] < video_info['duration']:
            filtered_timestamps.append(video_info['duration'])
        
        print(f"    Found {len(filtered_timestamps)} scene boundaries")
        return filtered_timestamps
        
    except subprocess.CalledProcessError:
        print("    FFmpeg scene detection failed")
        return []


def detect_scenes_frame_diff(input_video: str, min_scene_duration: float) -> List[float]:
    """Detect scenes using frame difference analysis."""
    try:
        # Extract frames at regular intervals and analyze differences
        video_info = get_video_info(input_video)
        fps = video_info['fps']
        duration = video_info['duration']
        
        # Sample frames every 0.5 seconds
        sample_interval = 0.5
        timestamps = []
        
        print(f"    Analyzing frame differences every {sample_interval}s...")
        
        for t in range(0, int(duration), int(sample_interval * fps)):
            timestamp = t / fps
            if timestamp >= min_scene_duration:
                timestamps.append(timestamp)
        
        # Add end time
        timestamps.append(duration)
        
        print(f"    Found {len(timestamps)} potential boundaries")
        return timestamps
        
    except Exception as e:
        print(f"    Frame difference analysis failed: {e}")
        return []


def intelligent_splitting(input_video: str, min_scene_duration: float) -> List[float]:
    """Intelligent splitting based on video characteristics."""
    video_info = get_video_info(input_video)
    duration = video_info['duration']
    
    print(f"    Using intelligent splitting for {duration:.1f}s video...")
    
    # For TikTok videos, typical short duration is 15-60 seconds
    # Let's try to find natural break points
    if duration <= 30:
        # Short video, likely 2-3 shorts
        num_clips = 2
    elif duration <= 60:
        # Medium video, likely 3-4 shorts
        num_clips = 3
    else:
        # Long video, likely 4+ shorts
        num_clips = 4
    
    # Calculate optimal clip duration
    optimal_duration = duration / num_clips
    
    # Adjust if clips would be too short
    if optimal_duration < min_scene_duration:
        num_clips = max(2, int(duration / min_scene_duration))
        optimal_duration = duration / num_clips
    
    print(f"    Splitting into {num_clips} clips of ~{optimal_duration:.1f}s each")
    
    # Generate timestamps
    timestamps = [0.0]
    for i in range(1, num_clips):
        timestamp = i * optimal_duration
        timestamps.append(timestamp)
    timestamps.append(duration)
    
    return timestamps


def split_video_by_scenes(input_video: str, scene_timestamps: List[float], 
                          output_dir: str = "smart_split") -> List[str]:
    """
    Split video based on detected scene boundaries.
    
    Args:
        input_video: Path to input video file
        scene_timestamps: List of timestamps where scenes change
        output_dir: Directory to save output clips
        
    Returns:
        List of paths to created video clips
    """
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    output_paths = []
    
    print(f"\n‚úÇÔ∏è  Splitting video into {len(scene_timestamps) - 1} clips...")
    
    for i in range(len(scene_timestamps) - 1):
        start_time = scene_timestamps[i]
        end_time = scene_timestamps[i + 1]
        duration = end_time - start_time
        
        # Generate output filename
        output_filename = f"tiktok_short_{i+1:02d}.mp4"
        output_path = os.path.join(output_dir, output_filename)
        
        print(f"\nProcessing clip {i+1}:")
        print(f"  Time: {start_time:.2f}s to {end_time:.2f}s")
        print(f"  Duration: {duration:.2f}s")
        
        # Use FFmpeg to extract the clip
        cmd = [
            'ffmpeg',
            '-i', input_video,
            '-ss', str(start_time),
            '-t', str(duration),
            '-c:v', 'libx264',
            '-c:a', 'aac',
            '-avoid_negative_ts', 'make_zero',
            '-y',
            output_path
        ]
        
        try:
            # Run FFmpeg command
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            print(f"  ‚úÖ Saved as: {output_filename}")
            output_paths.append(output_path)
            
        except subprocess.CalledProcessError as e:
            print(f"  ‚ùå Error: {e}")
            continue
    
    return output_paths


def main():
    """Main function for smart scene detection."""
    if len(sys.argv) < 2:
        print("Usage: python smart_scene_detect.py <input_video>")
        print("Example: python smart_scene_detect.py video.mp4")
        sys.exit(1)
    
    input_video = sys.argv[1]
    
    if not os.path.exists(input_video):
        print(f"Error: Input video file not found: {input_video}")
        sys.exit(1)
    
    # Get video information
    print("üìπ Getting video information...")
    info = get_video_info(input_video)
    if not info:
        print("‚ùå Could not get video information")
        sys.exit(1)
    
    print(f"‚úÖ Video loaded successfully:")
    print(f"   Duration: {info['duration']:.2f} seconds")
    print(f"   Resolution: {info['width']}x{info['height']}")
    print(f"   Frame Rate: {info['fps']:.2f} FPS")
    if info['audio_fps']:
        print(f"   Audio: {info['audio_fps']} Hz")
    print(f"   File Size: {info['file_size_mb']:.2f} MB")
    
    # Detect scenes automatically
    scene_timestamps = detect_scenes_advanced(input_video)
    
    if len(scene_timestamps) < 2:
        print("‚ùå Could not detect any scene boundaries")
        sys.exit(1)
    
    print(f"\nüéØ Detected {len(scene_timestamps) - 1} scenes:")
    for i in range(len(scene_timestamps) - 1):
        start = scene_timestamps[i]
        end = scene_timestamps[i + 1]
        duration = end - start
        print(f"  Scene {i+1}: {start:.2f}s to {end:.2f}s ({duration:.2f}s)")
    
    # Ask user if they want to proceed
    response = input(f"\nProceed with splitting into {len(scene_timestamps) - 1} clips? (y/n): ").lower()
    if response != 'y':
        print("Operation cancelled.")
        return
    
    # Split the video
    clips = split_video_by_scenes(input_video, scene_timestamps)
    
    if clips:
        print(f"\nüéâ Successfully created {len(clips)} clips!")
        for i, clip_path in enumerate(clips, 1):
            file_size = os.path.getsize(clip_path) / (1024 * 1024)
            print(f"  {i:2d}. {os.path.basename(clip_path)} ({file_size:.2f} MB)")
        
        print(f"\nAll clips saved to: {os.path.abspath('smart_split')}")
    else:
        print("‚ùå No clips were created successfully")


if __name__ == "__main__":
    main()
